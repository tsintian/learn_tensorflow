{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eager Execution\n",
    "\n",
    "> Tensorflow's eager execution is an imperative programming environment that evalutes operations immediately, without building graphs: operations return concrete values instead of constructing a computational graph to run later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tianqin/anaconda3/envs/tensorflow/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "# Set up eager mode.\n",
    "import tensorflow as tf\n",
    "\n",
    "tf.enable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check whether it is running in eager mode.\n",
    "tf.executing_eagerly()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello, [[4.]]\n"
     ]
    }
   ],
   "source": [
    "x = [[2.]]\n",
    "m = tf.matmul(x, x)\n",
    "print(\"hello, {}\".format(m))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, enabling eager execution changed how Tensorflow operations behave:\n",
    "\n",
    "1. Now we do not create a Seession first and following by been executed in a graph, instead, we can execute it directly.\n",
    "\n",
    "2. Operations are immediately executed and returned their vales to Python.\n",
    "\n",
    "3. `tf.Tensor` objects reference concrete values instead of symbolic handles to nodes in a computational graph."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eager execution works nicely with Numpy\n",
    "\n",
    "- Numpy operations accept `tf.Tensor` arguments.\n",
    "- Tensorflow math operations convert Python objects and `tf.Tensor` objects.\n",
    "- The `tf.Tensor.numpy` method returns the object's value as a Numpy `ndarray`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[1 2]\n",
      " [3 4]], shape=(2, 2), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "a = tf.constant([[1,2], [3,4]])\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[2 3]\n",
      " [4 5]], shape=(2, 2), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "b = tf.add(a, 1)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[ 2  6]\n",
      " [12 20]], shape=(2, 2), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "print(a * b)  # Operator overloading\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 2  6]\n",
      " [12 20]]\n"
     ]
    }
   ],
   "source": [
    "# Numpy operators can accept tf.Tensor object directly\n",
    "import numpy as np\n",
    "\n",
    "c = np.multiply(a, b)\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 2]\n",
      " [3 4]]\n"
     ]
    }
   ],
   "source": [
    "# Convert tf.Tensor object to numpy value.\n",
    "print(a.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `tf.contrib.eager` module contains symbols available to both eager and graph execution environments and is useful for writing code to work with graphs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tfe = tf.contrib.eager"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a model\n",
    "\n",
    "When composing layers into models we can use `tf.keras.Sequential` to represent models which are a linear stack of layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(10, input_shape=(784,)), # input is needed.\n",
    "    tf.keras.layers.Dense(10)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, organise models in classes by inheriting from `tf.keras.Model`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MNISTModel(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(MNISTModel, self).__init__()\n",
    "        self.dense1 = tf.keras.layers.Dense(units = 10)\n",
    "        self.dense2 = tf.keras.layers.Dense(units = 10)\n",
    "    \n",
    "    def call(self, input):\n",
    "        \"\"\"Run the model\"\"\"\n",
    "        result = self.dense1(input)\n",
    "        result = self.dense2(result)\n",
    "        result = self.dense2(result)\n",
    "        return result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eager training\n",
    "-------------------------------------------------------\n",
    "### Computing gradients\n",
    "\n",
    "During eager execution, use `tf.GradientTape` to trace operations for computing gradients later.\n",
    "\n",
    "A particular `tf.GradientTape` can only compute one gradient; subsequent calls throw a runtime error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Tensor: id=78, shape=(1, 1), dtype=float32, numpy=array([[2.]], dtype=float32)>]\n"
     ]
    }
   ],
   "source": [
    "w = tfe.Variable([[1.0]])\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "    loss = w * w\n",
    "\n",
    "grad = tape.gradient(loss, [w])\n",
    "print(grad)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example: use `tf.GraidentTape` to record forward-pass operations to train a simple model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial loss: 69.130341\n",
      "Loss at step   0 : 66.461044\n",
      "Loss at step  20 : 30.433773\n",
      "Loss at step  40 : 14.233822\n",
      "Loss at step  60 : 6.947694\n",
      "Loss at step  80 : 3.669947\n",
      "Loss at step 100 : 2.195095\n",
      "Loss at step 120 : 1.531334\n",
      "Loss at step 140 : 1.232548\n",
      "Loss at step 160 : 1.098025\n",
      "Loss at step 180 : 1.037447\n",
      "Final loss: 1.011073\n",
      "W = 3.000185489654541, B=2.1008214950561523\n"
     ]
    }
   ],
   "source": [
    "# A toy dataset of points around 3 * x + 2\n",
    "NUM_EXAMPLES = 1000\n",
    "training_inputs = tf.random_normal([NUM_EXAMPLES])\n",
    "noise = tf.random_normal([NUM_EXAMPLES])\n",
    "\n",
    "training_outputs = training_inputs * 3 + 2 + noise\n",
    "\n",
    "def prediction(input, weight, bias):\n",
    "    return input* weight + bias\n",
    "\n",
    "# A loss functioin using mean-squared error\n",
    "def loss(weights, biases):\n",
    "    error = prediction(training_inputs, weights, biases) - training_outputs\n",
    "    return tf.reduce_mean(tf.square(error))\n",
    "\n",
    "# Return the derivative of loss with respect to weight and bias\n",
    "def grad(weights, biases):\n",
    "    with tf.GradientTape() as tape:\n",
    "        loss_value = loss(weights, biases)\n",
    "    return tape.gradient(loss_value, [weights, biases])\n",
    "    \n",
    "train_steps = 200\n",
    "learning_rate = 0.01\n",
    "\n",
    "# Start with arbitrary values for W and B\n",
    "W = tfe.Variable(5.)\n",
    "B = tfe.Variable(10.)\n",
    "\n",
    "print(\"Initial loss: {:3f}\".format(loss(W, B)))\n",
    "\n",
    "for i in range(train_steps):\n",
    "    dW, dB = grad(W, B)\n",
    "    W.assign_sub(dW * learning_rate)\n",
    "    B.assign_sub(dB * learning_rate)\n",
    "    if i % 20 == 0:\n",
    "        print(\"Loss at step {:3d} : {:3f}\".format(i, loss(W, B)))\n",
    "\n",
    "print(\"Final loss: {:3f}\".format(loss(W, B)))\n",
    "print(\"W = {}, B={}\".format(W.numpy(), B.numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow]",
   "language": "python",
   "name": "conda-env-tensorflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
