{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-18T14:36:50.926606Z",
     "start_time": "2019-06-18T14:36:50.849249Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "深度学习模型不会接受原始文本所为输入，它只能吃力数值张量。文本向量化(Vectorize)是指将文本转换为张量的过程。\n",
    "\n",
    "文本向量化有多种实现方式：\n",
    "\n",
    "- 将文本分割为单词，并将每个单词转换为一个向量\n",
    "- 将文本分割为字符，并将每个字符转换为一个向量\n",
    "- 提取单词或者字符的n-gram,并将每个n-gram转换为一个向量。n-gram是多个连续单词或者字符的结合(n-gram间可以重叠)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 单词和字符的one-hot编码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-18T14:36:50.959284Z",
     "start_time": "2019-06-18T14:36:50.929902Z"
    }
   },
   "outputs": [],
   "source": [
    "# 单词级别的one-hot编码\n",
    "samples = ['The cat sat on the mat.', 'The dog ate my homework.']\n",
    "token_index = {}\n",
    "for sample in samples:\n",
    "    for word in sample.split():\n",
    "        if word not in token_index:\n",
    "            token_index[word] = len(token_index) + 1 \n",
    "max_length = 10 \n",
    "results = np.zeros(shape=(len(samples), \n",
    "                             max_length,\n",
    "                             max(token_index.values())+1 ))\n",
    "\n",
    "# 将结果保存在results里面\n",
    "for i ,sample in enumerate(samples):\n",
    "    for j, word in list(enumerate(sample.split()))[:max_length]:\n",
    "        index = token_index.get(word)\n",
    "        results[i,j,index]=1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-18T14:36:51.041062Z",
     "start_time": "2019-06-18T14:36:50.962399Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
       "\n",
       "       [[0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-18T14:36:52.473143Z",
     "start_time": "2019-06-18T14:36:51.043667Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# 使用keras实现one-hot编码\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "samples = ['The cat sat on the mat.', 'The dog ate my homework.']\n",
    "tokenizer = Tokenizer(num_words=1000)\n",
    "# 构建单词索引\n",
    "tokenizer.fit_on_texts(samples)\n",
    "# 将字符串转换为整数索引组成的列表\n",
    "sequences = tokenizer.texts_to_sequences(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-18T14:36:52.479015Z",
     "start_time": "2019-06-18T14:36:52.474850Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.document_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-18T14:36:52.562575Z",
     "start_time": "2019-06-18T14:36:52.481048Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'the': 1,\n",
       " 'cat': 2,\n",
       " 'sat': 3,\n",
       " 'on': 4,\n",
       " 'mat': 5,\n",
       " 'dog': 6,\n",
       " 'ate': 7,\n",
       " 'my': 8,\n",
       " 'homework': 9}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-18T14:36:52.638322Z",
     "start_time": "2019-06-18T14:36:52.564827Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 2, 3, 4, 1, 5], [1, 6, 7, 8, 9]]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-18T14:36:52.711556Z",
     "start_time": "2019-06-18T14:36:52.642939Z"
    }
   },
   "outputs": [],
   "source": [
    "# 直接得到one-hot二进制表示\n",
    "one_hot_results = tokenizer.texts_to_matrix(samples, mode='freq')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-18T14:36:52.789739Z",
     "start_time": "2019-06-18T14:36:52.715722Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.33333333, 0.16666667, ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.2       , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_hot_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-18T14:36:52.863000Z",
     "start_time": "2019-06-18T14:36:52.792949Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 1000)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_hot_results.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-18T14:36:52.945014Z",
     "start_time": "2019-06-18T14:36:52.865774Z"
    }
   },
   "outputs": [],
   "source": [
    "# 加载IMDB数据\n",
    "from keras.datasets import imdb\n",
    "from keras import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-18T14:36:57.137319Z",
     "start_time": "2019-06-18T14:36:52.948091Z"
    }
   },
   "outputs": [],
   "source": [
    "max_features = 10000 \n",
    "maxlen = 20 \n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-18T14:36:57.144350Z",
     "start_time": "2019-06-18T14:36:57.139616Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((25000,), (25000,))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-18T14:36:57.251047Z",
     "start_time": "2019-06-18T14:36:57.145976Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(218, 189)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x_train[0]), len(x_train[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-18T14:36:57.674073Z",
     "start_time": "2019-06-18T14:36:57.253845Z"
    }
   },
   "outputs": [],
   "source": [
    "x_train = preprocessing.sequence.pad_sequences(x_train, maxlen=maxlen)\n",
    "x_test = preprocessing.sequence.pad_sequences(x_test, maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-18T14:36:57.679749Z",
     "start_time": "2019-06-18T14:36:57.675873Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 20)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-18T14:36:57.761915Z",
     "start_time": "2019-06-18T14:36:57.681715Z"
    }
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Flatten, Dense, Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-18T14:36:58.166117Z",
     "start_time": "2019-06-18T14:36:57.764749Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/qintian/anaconda3/envs/deep/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(1000, 8, input_length=maxlen))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(optimizer='rmsprop', loss='binary_crossentropy',\n",
    "             metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-18T14:36:58.174009Z",
     "start_time": "2019-06-18T14:36:58.168150Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 20, 8)             8000      \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 160)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 161       \n",
      "=================================================================\n",
      "Total params: 8,161\n",
      "Trainable params: 8,161\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-18T14:37:35.856912Z",
     "start_time": "2019-06-18T14:36:58.175432Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/qintian/anaconda3/envs/deep/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 20000 samples, validate on 5000 samples\n",
      "Epoch 1/10\n",
      "20000/20000 [==============================] - 12s 597us/step - loss: 0.6767 - acc: 0.6026 - val_loss: 0.6427 - val_acc: 0.6696\n",
      "Epoch 2/10\n",
      "20000/20000 [==============================] - 3s 142us/step - loss: 0.5796 - acc: 0.7205 - val_loss: 0.5606 - val_acc: 0.7094\n",
      "Epoch 3/10\n",
      "20000/20000 [==============================] - 3s 143us/step - loss: 0.5128 - acc: 0.7469 - val_loss: 0.5355 - val_acc: 0.7220\n",
      "Epoch 4/10\n",
      "20000/20000 [==============================] - 3s 137us/step - loss: 0.4868 - acc: 0.7621 - val_loss: 0.5302 - val_acc: 0.7270\n",
      "Epoch 5/10\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 0.4728 - acc: 0.7699 - val_loss: 0.5292 - val_acc: 0.7344\n",
      "Epoch 6/10\n",
      "20000/20000 [==============================] - 3s 140us/step - loss: 0.4635 - acc: 0.7763 - val_loss: 0.5323 - val_acc: 0.7328\n",
      "Epoch 7/10\n",
      "20000/20000 [==============================] - 3s 141us/step - loss: 0.4553 - acc: 0.7817 - val_loss: 0.5337 - val_acc: 0.7312\n",
      "Epoch 8/10\n",
      "20000/20000 [==============================] - 3s 134us/step - loss: 0.4478 - acc: 0.7861 - val_loss: 0.5384 - val_acc: 0.7258\n",
      "Epoch 9/10\n",
      "20000/20000 [==============================] - 3s 142us/step - loss: 0.4407 - acc: 0.7907 - val_loss: 0.5424 - val_acc: 0.7278\n",
      "Epoch 10/10\n",
      "20000/20000 [==============================] - 3s 143us/step - loss: 0.4338 - acc: 0.7938 - val_loss: 0.5461 - val_acc: 0.7280\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(x_train,y_train, epochs=10, batch_size=32, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-18T14:37:35.865392Z",
     "start_time": "2019-06-18T14:37:35.860855Z"
    }
   },
   "outputs": [],
   "source": [
    "# 使用预训练的词嵌入\n",
    "import os "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-18T14:37:35.953634Z",
     "start_time": "2019-06-18T14:37:35.867518Z"
    }
   },
   "outputs": [],
   "source": [
    "imdb_dir = '../../data/imdb/aclImdb/'\n",
    "train_dir = os.path.join(imdb_dir, 'train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-18T14:37:36.792134Z",
     "start_time": "2019-06-18T14:37:35.959535Z"
    }
   },
   "outputs": [],
   "source": [
    "labels = []\n",
    "texts = []\n",
    "\n",
    "for label_type in ['neg', 'pos']:\n",
    "    dir_name = os.path.join(train_dir, label_type)\n",
    "    for fname in os.listdir(dir_name):\n",
    "        if fname[-4:] == '.txt':\n",
    "            with open(os.path.join(dir_name, fname)) as f:\n",
    "                texts.append(f.read())\n",
    "                if label_type == 'neg':\n",
    "                    labels.append(0)\n",
    "                else:\n",
    "                    labels.append(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-18T14:37:36.798305Z",
     "start_time": "2019-06-18T14:37:36.795456Z"
    }
   },
   "outputs": [],
   "source": [
    "from keras import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-18T14:37:36.878738Z",
     "start_time": "2019-06-18T14:37:36.799849Z"
    }
   },
   "outputs": [],
   "source": [
    "maxlen = 100 \n",
    "training_samples = 200 \n",
    "validation_samples = 10000 \n",
    "max_words = 10000 \n",
    "\n",
    "# 只考虑数据集中前10000个最常见的单词\n",
    "tokenizer =  preprocessing.text.Tokenizer(num_words=max_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-18T14:37:36.964138Z",
     "start_time": "2019-06-18T14:37:36.881327Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25000"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-18T14:37:40.865036Z",
     "start_time": "2019-06-18T14:37:36.967259Z"
    }
   },
   "outputs": [],
   "source": [
    "tokenizer.fit_on_texts(texts=texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-18T14:37:43.643744Z",
     "start_time": "2019-06-18T14:37:40.866711Z"
    }
   },
   "outputs": [],
   "source": [
    "sequences = tokenizer.texts_to_sequences(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-18T14:37:43.651600Z",
     "start_time": "2019-06-18T14:37:43.646183Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25000"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-18T14:37:43.983842Z",
     "start_time": "2019-06-18T14:37:43.653011Z"
    }
   },
   "outputs": [],
   "source": [
    "data = preprocessing.sequence.pad_sequences(sequences, maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-18T14:37:43.996067Z",
     "start_time": "2019-06-18T14:37:43.985581Z"
    }
   },
   "outputs": [],
   "source": [
    "labels = np.asarray(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-18T14:37:44.087822Z",
     "start_time": "2019-06-18T14:37:43.997922Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000,)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-18T14:37:44.171411Z",
     "start_time": "2019-06-18T14:37:44.089641Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 100)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-18T14:37:44.253674Z",
     "start_time": "2019-06-18T14:37:44.173666Z"
    }
   },
   "outputs": [],
   "source": [
    "indices = np.arange(data.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-18T14:37:44.338567Z",
     "start_time": "2019-06-18T14:37:44.256454Z"
    }
   },
   "outputs": [],
   "source": [
    "# 将数据划分为训练集和验证集，需要打乱数据\n",
    "np.random.shuffle(indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-18T14:37:44.443428Z",
     "start_time": "2019-06-18T14:37:44.341657Z"
    }
   },
   "outputs": [],
   "source": [
    "data = data[indices]\n",
    "labels = labels[indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-18T14:37:44.512683Z",
     "start_time": "2019-06-18T14:37:44.445275Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200, 10000)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_samples, validation_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-18T14:37:44.595400Z",
     "start_time": "2019-06-18T14:37:44.514448Z"
    }
   },
   "outputs": [],
   "source": [
    "x_train = data[:training_samples]\n",
    "y_train = labels[:training_samples]\n",
    "\n",
    "x_val = data[training_samples: training_samples + validation_samples]\n",
    "y_val = labels[training_samples: training_samples + validation_samples]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 对词嵌入进行预处理\n",
    "\n",
    "对解压后的文件进行解析，构建一个将单词映射成其向量表示的索引"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-18T14:37:54.769776Z",
     "start_time": "2019-06-18T14:37:44.597898Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400000\n"
     ]
    }
   ],
   "source": [
    "glove_file = '../../data/glove.6B.100d.txt'\n",
    "embeddings_index = {}\n",
    "i = 0 \n",
    "with open(glove_file, 'r') as f:\n",
    "    for line in f:\n",
    "        i += 1\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = coefs\n",
    "        \n",
    "print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-18T14:57:01.656606Z",
     "start_time": "2019-06-18T14:57:01.650570Z"
    }
   },
   "outputs": [],
   "source": [
    "word_index = tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-06-18T14:58:16.585Z"
    }
   },
   "outputs": [],
   "source": [
    "# 构建一个可以加载到Embedding层中的嵌入矩阵\n",
    "# 其形状为(max_words, embedding_dim)\n",
    "\n",
    "embedding_dim = 100 \n",
    "embedding_matrix = np.zeros((max_words, embedding_dim))\n",
    "\n",
    "for embedding_vector, i in word_index.items():\n",
    "    if i < max_words:\n",
    "        embedding_vector = embedding_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-06-18T15:00:11.280Z"
    }
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "fromrom keras.layers import Embedding, Flatten, Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
